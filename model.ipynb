{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation Model To Detect Stones In The Image \n",
    "## A breif documentation regarding this model has been provided alongside this .ipynb file. Please refer to that documentation for better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries\n",
    "    While importing segmentation_models module, you can encounter \"keras.engine not found\" error, to fix that please use \"os.environ['SM_FRAMEWORK'] = 'tf.keras' \"\n",
    "    The respective version of each library are given alongside the import statements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ther versions have been mentioned alongside the import statements\n",
    "import os\n",
    "import numpy as np # 1.25.2\n",
    "import cv2 # 4.9.0\n",
    "import random\n",
    "import albumentations as A # 1.4.1\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "import segmentation_models as sm # 1.0.1\n",
    "from segmentation_models import get_preprocessing\n",
    "import sklearn #1.4.1.post1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt # 3.7.2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras # 3.0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains the images and labels provided in the coding challenge\n",
    "datasetPath = './Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the path to images and labels in respective lists\n",
    "\n",
    "imagesList = []\n",
    "labelsList = []\n",
    "\n",
    "for file in os.listdir(datasetPath):\n",
    "    # image file end with \".JPG\" where as label files end with \".jpg\" extension\n",
    "    if file.endswith('.JPG'):\n",
    "        imagesList.append(os.path.join(datasetPath, file))\n",
    "        label = file.split('.')[0] + '_label.jpg'\n",
    "        labelsList.append(os.path.join(datasetPath, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List containing path of images is :  ['./Dataset/Image14.JPG', './Dataset/Image15.JPG', './Dataset/Image01.JPG', './Dataset/Image17.JPG', './Dataset/Image03.JPG', './Dataset/Image02.JPG', './Dataset/Image16.JPG', './Dataset/Image12.JPG', './Dataset/Image06.JPG', './Dataset/Image07.JPG', './Dataset/Image13.JPG', './Dataset/Image05.JPG', './Dataset/Image11.JPG', './Dataset/Image10.JPG', './Dataset/Image04.JPG', './Dataset/Image09.JPG', './Dataset/Image08.JPG', './Dataset/Image20.JPG', './Dataset/Image18.JPG', './Dataset/Image19.JPG']\n",
      "Type of content in image list is :  <class 'str'>\n",
      "List containign path of labels is :  ['./Dataset/Image14_label.jpg', './Dataset/Image15_label.jpg', './Dataset/Image01_label.jpg', './Dataset/Image17_label.jpg', './Dataset/Image03_label.jpg', './Dataset/Image02_label.jpg', './Dataset/Image16_label.jpg', './Dataset/Image12_label.jpg', './Dataset/Image06_label.jpg', './Dataset/Image07_label.jpg', './Dataset/Image13_label.jpg', './Dataset/Image05_label.jpg', './Dataset/Image11_label.jpg', './Dataset/Image10_label.jpg', './Dataset/Image04_label.jpg', './Dataset/Image09_label.jpg', './Dataset/Image08_label.jpg', './Dataset/Image20_label.jpg', './Dataset/Image18_label.jpg', './Dataset/Image19_label.jpg']\n",
      "Type of content in label list is :  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# checking the type of content in the lists\n",
    "\n",
    "print(\"List containing path of images is : \", imagesList)\n",
    "print(\"Type of content in image list is : \", type(imagesList[0]))\n",
    "\n",
    "print(\"List containign path of labels is : \", labelsList)\n",
    "print(\"Type of content in label list is : \",type(labelsList[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "We are making use of the albumentations library to generate augmented images since originally our dataset contains only 20 input images and 20 labels.\n",
    "##### Transformations used for augmenting image : \n",
    "                                        1. Rotation (probability : 50%)\n",
    "                                        2. Horizontal flipping (probability : 50%)\n",
    "                                        3. Vertical fliping (probability : 50%) \n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Rotate(limit=30, p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# directories to store generated images and labels\n",
    "transformed_images_dir = \"./Data/Images\"\n",
    "transformed_labels_dir = \"./Data/Labels\"\n",
    "\n",
    "images_to_generate = 300\n",
    "i = 1\n",
    "while i <= images_to_generate:\n",
    "    #randomly choose an image from available dataset to perform transformation\n",
    "    number = random.randint(0, len(imagesList)-1)\n",
    "    image = imagesList[number]\n",
    "    label = labelsList[number]\n",
    "    originalImage = cv2.imread(image)\n",
    "    originalImage = cv2.cvtColor(originalImage, cv2.COLOR_BGR2RGB)\n",
    "    originalLabel = cv2.imread(label)\n",
    "    originalLabel = cv2.cvtColor(originalLabel, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    transformed = transform( image=originalImage , mask=originalLabel)\n",
    "    transformed_image = transformed['image']\n",
    "    transformed_label = transformed['mask']\n",
    "\n",
    "    transformed_image_path = os.path.join(transformed_images_dir, f'Image{i}_.JPG')\n",
    "    transformed_label_path = os.path.join(transformed_labels_dir, f'Image{i}_label_.jpg')\n",
    "\n",
    "    # save the augmented image and label to respective directories\n",
    "    cv2.imwrite(transformed_image_path, transformed_image)\n",
    "    cv2.imwrite(transformed_label_path, transformed_label)\n",
    "\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_images_dir = \"./Data/Images\"\n",
    "transformed_labels_dir = \"./Data/Labels\"\n",
    "\n",
    "# list to store the augmented images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for file in os.listdir('./Data/Images'):\n",
    "    #choose a image and find it's respective label\n",
    "    img_path = './Data/Images/' + file\n",
    "    l = file.split('_')[0] +'_label_.jpg'\n",
    "    lbl_path = './Data/Labels/' + l\n",
    "\n",
    "    # input image is RGB\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img , cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img , (256 , 256))\n",
    "    images.append(img)\n",
    "\n",
    "    # label is GRAYSCALE\n",
    "    lbl = cv2.imread(lbl_path)\n",
    "    lbl = cv2.cvtColor(lbl, cv2.COLOR_BGR2GRAY)\n",
    "    lbl = cv2.resize(lbl , (256 , 256))\n",
    "    labels.append(lbl)\n",
    "\n",
    "# convert list to numpy array to be able to be used by the deep learning model\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Images Numpy Array ----\n",
      "Shape of array :  (300, 256, 256, 3)\n",
      "Shape of every image :  (256, 256, 3)\n",
      "\n",
      "\n",
      "---- Labels Numpy Array ----\n",
      "Shape of array :  (300, 256, 256)\n",
      "Shape of every label :  (256, 256)\n"
     ]
    }
   ],
   "source": [
    "# verifying the shapes of the input and label arrays\n",
    "print(\"---- Images Numpy Array ----\")\n",
    "print(\"Shape of array : \", images.shape)\n",
    "print(\"Shape of every image : \", images[3].shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"---- Labels Numpy Array ----\")\n",
    "print(\"Shape of array : \", labels.shape)\n",
    "print(\"Shape of every label : \", labels[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 256, 256, 1)\n",
      "(256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# adding channel dimension to the grayscale labels so that these are useable in the model\n",
    "labels = np.expand_dims( labels , axis=-1)\n",
    "print(labels.shape)\n",
    "print(labels[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prerequisites\n",
    "    We will be using the resnet18 backbone for creating our model coupled with the Unet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'resnet18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = get_preprocessing(BACKBONE)\n",
    "\n",
    "x= images\n",
    "y= labels\n",
    "\n",
    "# converting the pixel value to be in the range of 0 to 1 and of float32 type\n",
    "x = x.astype(np.float32)/255.0\n",
    "y = y.astype(np.float32)/255.0\n",
    "\n",
    "# splitting the dataset into training and testing part, getting a 80-20 split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y , test_size=0.2, random_state=42)\n",
    "x_train = preprocess_input(x_train)\n",
    "x_val = preprocess_input(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Checking Shapes ----\n",
      "x_train :  (240, 256, 256, 3)\n",
      "x_val :  (60, 256, 256, 3)\n",
      "y_train :  (240, 256, 256, 1)\n",
      "y_val :  (60, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"---- Checking Shapes ----\")\n",
    "print(\"x_train : \", x_train.shape)\n",
    "print(\"x_val : \", x_val.shape)\n",
    "print(\"y_train : \", y_train.shape)\n",
    "print(\"y_val : \", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "    We will use the Unet architecture provided by the semantic_models library.\n",
    "    Since we only want to segment stones from the image, we will use classes = 1, and a sigmiod activation function.\n",
    "    In additon to this , the encoder_weights are set to None therefore we will calulate weights for both the encoder and decoder part ourself.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Unet(BACKBONE, classes = 1, activation='sigmoid' ,encoder_weights=None)\n",
    "model.compile(\n",
    "    'Adam',\n",
    "    loss=sm.losses.binary_crossentropy,\n",
    "    metrics=[sm.metrics.iou_score],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 887ms/step - iou_score: 0.2641 - loss: 0.0575 - val_iou_score: 0.1426 - val_loss: 0.0836\n",
      "Epoch 2/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 906ms/step - iou_score: 0.4598 - loss: 0.0280 - val_iou_score: 0.3433 - val_loss: 0.0374\n",
      "Epoch 3/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 912ms/step - iou_score: 0.5678 - loss: 0.0208 - val_iou_score: 0.5033 - val_loss: 0.0236\n",
      "Epoch 4/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 915ms/step - iou_score: 0.6301 - loss: 0.0175 - val_iou_score: 0.5923 - val_loss: 0.0193\n",
      "Epoch 5/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 910ms/step - iou_score: 0.6594 - loss: 0.0159 - val_iou_score: 0.6372 - val_loss: 0.0169\n",
      "Epoch 6/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 913ms/step - iou_score: 0.6863 - loss: 0.0145 - val_iou_score: 0.6640 - val_loss: 0.0150\n",
      "Epoch 7/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 911ms/step - iou_score: 0.7073 - loss: 0.0139 - val_iou_score: 0.6926 - val_loss: 0.0139\n",
      "Epoch 8/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 911ms/step - iou_score: 0.7237 - loss: 0.0126 - val_iou_score: 0.7095 - val_loss: 0.0125\n",
      "Epoch 9/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 915ms/step - iou_score: 0.7309 - loss: 0.0123 - val_iou_score: 0.7222 - val_loss: 0.0124\n",
      "Epoch 10/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 918ms/step - iou_score: 0.7450 - loss: 0.0119 - val_iou_score: 0.7328 - val_loss: 0.0120\n",
      "Epoch 11/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 916ms/step - iou_score: 0.7534 - loss: 0.0112 - val_iou_score: 0.7471 - val_loss: 0.0120\n",
      "Epoch 12/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 915ms/step - iou_score: 0.7568 - loss: 0.0113 - val_iou_score: 0.7556 - val_loss: 0.0117\n",
      "Epoch 13/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 936ms/step - iou_score: 0.7732 - loss: 0.0103 - val_iou_score: 0.7552 - val_loss: 0.0110\n",
      "Epoch 14/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 928ms/step - iou_score: 0.7785 - loss: 0.0104 - val_iou_score: 0.7364 - val_loss: 0.0117\n",
      "Epoch 15/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 909ms/step - iou_score: 0.7873 - loss: 0.0100 - val_iou_score: 0.7743 - val_loss: 0.0102\n",
      "Epoch 16/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 903ms/step - iou_score: 0.7964 - loss: 0.0091 - val_iou_score: 0.7781 - val_loss: 0.0103\n",
      "Epoch 17/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 905ms/step - iou_score: 0.8068 - loss: 0.0088 - val_iou_score: 0.7896 - val_loss: 0.0098\n",
      "Epoch 18/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 898ms/step - iou_score: 0.8103 - loss: 0.0087 - val_iou_score: 0.7946 - val_loss: 0.0098\n",
      "Epoch 19/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 896ms/step - iou_score: 0.8151 - loss: 0.0084 - val_iou_score: 0.8006 - val_loss: 0.0098\n",
      "Epoch 20/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 896ms/step - iou_score: 0.8189 - loss: 0.0081 - val_iou_score: 0.7920 - val_loss: 0.0097\n",
      "Epoch 21/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 902ms/step - iou_score: 0.8230 - loss: 0.0080 - val_iou_score: 0.8138 - val_loss: 0.0091\n",
      "Epoch 22/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 894ms/step - iou_score: 0.8316 - loss: 0.0075 - val_iou_score: 0.6363 - val_loss: 0.0333\n",
      "Epoch 23/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 900ms/step - iou_score: 0.7858 - loss: 0.0104 - val_iou_score: 0.8128 - val_loss: 0.0096\n",
      "Epoch 24/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 941ms/step - iou_score: 0.8264 - loss: 0.0079 - val_iou_score: 0.8155 - val_loss: 0.0090\n",
      "Epoch 25/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - iou_score: 0.8345 - loss: 0.0074 - val_iou_score: 0.8147 - val_loss: 0.0089\n",
      "Epoch 26/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - iou_score: 0.8433 - loss: 0.0069 - val_iou_score: 0.8250 - val_loss: 0.0086\n",
      "Epoch 27/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - iou_score: 0.8460 - loss: 0.0069 - val_iou_score: 0.8292 - val_loss: 0.0084\n",
      "Epoch 28/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - iou_score: 0.8527 - loss: 0.0065 - val_iou_score: 0.8294 - val_loss: 0.0084\n",
      "Epoch 29/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - iou_score: 0.8525 - loss: 0.0065 - val_iou_score: 0.8290 - val_loss: 0.0087\n",
      "Epoch 30/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - iou_score: 0.8579 - loss: 0.0065 - val_iou_score: 0.8329 - val_loss: 0.0086\n",
      "Epoch 31/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - iou_score: 0.8601 - loss: 0.0062 - val_iou_score: 0.8340 - val_loss: 0.0085\n",
      "Epoch 32/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - iou_score: 0.8629 - loss: 0.0060 - val_iou_score: 0.8414 - val_loss: 0.0083\n",
      "Epoch 33/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - iou_score: 0.8638 - loss: 0.0060 - val_iou_score: 0.8378 - val_loss: 0.0085\n",
      "Epoch 34/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - iou_score: 0.8665 - loss: 0.0059 - val_iou_score: 0.8423 - val_loss: 0.0083\n",
      "Epoch 35/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 1s/step - iou_score: 0.8687 - loss: 0.0057 - val_iou_score: 0.8447 - val_loss: 0.0083\n",
      "Epoch 36/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - iou_score: 0.8713 - loss: 0.0057 - val_iou_score: 0.8409 - val_loss: 0.0084\n",
      "Epoch 37/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - iou_score: 0.8719 - loss: 0.0056 - val_iou_score: 0.8465 - val_loss: 0.0083\n",
      "Epoch 38/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 1s/step - iou_score: 0.8717 - loss: 0.0055 - val_iou_score: 0.8444 - val_loss: 0.0085\n",
      "Epoch 39/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - iou_score: 0.8766 - loss: 0.0052 - val_iou_score: 0.8483 - val_loss: 0.0085\n",
      "Epoch 40/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - iou_score: 0.8784 - loss: 0.0054 - val_iou_score: 0.8455 - val_loss: 0.0084\n",
      "Epoch 41/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - iou_score: 0.8792 - loss: 0.0053 - val_iou_score: 0.8520 - val_loss: 0.0085\n",
      "Epoch 42/50\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - iou_score: 0.8781 - loss: 0.0054 - val_iou_score: 0.8495 - val_loss: 0.0084\n",
      "Epoch 42: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# early stopping help in stopping the training process before all the epochs have completed in case there is no signifiant improvement in loss\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# calculating the weights and creating the model\n",
    "model.fit(\n",
    "    x = x_train,\n",
    "    y = y_train,\n",
    "    batch_size = 4,\n",
    "    epochs = 50,\n",
    "    validation_data = (x_val, y_val),\n",
    "    callbacks = [early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model for future predictions\n",
    "model.save('stone_seg_resnet18_encoderNone.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =keras.models.load_model('./stone_seg_resnet18_encoderNone.keras', compile=False)\n",
    "\n",
    "test_img = cv2.imread('./Dataset/Image01.JPG')\n",
    "test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "test_img = cv2.resize(test_img, (256 , 256))\n",
    "test_img = test_img.astype(np.float32)/255.0\n",
    "test_img = np.expand_dims(test_img, axis=0)\n",
    "\n",
    "prediction = model.predict(test_img)\n",
    "pred_img = np.squeeze(prediction , axis=0)\n",
    "plt.imshow(pred_img , cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
